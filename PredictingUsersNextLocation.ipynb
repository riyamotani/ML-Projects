{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdAqjQOa8HdJ",
        "outputId": "33fc36ed-365d-4862-ff10-a34aad3e3dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time cost: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def preprocessing():\n",
        "    params = {\n",
        "        'path': '/content/dataset_TSMC2014_NYC.csv',\n",
        "        'dataname': 'NYC',\n",
        "        'filetype': 'csv',\n",
        "        'user_po': 'userId',\n",
        "        'loc_po': 'venueId',\n",
        "        'tim_po': 'utcTimestamp',\n",
        "        'user_record_min': 10,\n",
        "        'loc_record_min': 10\n",
        "    }\n",
        "\n",
        "    # Loading and Filtering sparse data\n",
        "    print('='*20, 'Loading and preprocessing sparse data')\n",
        "\n",
        "    # Specify the path of the dataset\n",
        "    filepath = f'/content/dataset_TSMC2014_NYC.csv'\n",
        "    print(f'Path is {filepath}')\n",
        "\n",
        "    loc_count = {}  # store location info with loc-num\n",
        "    user_count = {}  # store user info with user-num\n",
        "    user_id = {}\n",
        "    loc_id = {}\n",
        "\n",
        "    # Load file and count numbers\n",
        "    print('='*20, 'Loading and Counting')\n",
        "    if params['filetype'] == 'csv':\n",
        "        with open(filepath, 'r', encoding='latin-1') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for record in reader:\n",
        "                user = record[params['user_po']]\n",
        "                loc = record[params['loc_po']]\n",
        "\n",
        "                # Count the occurrences of each user\n",
        "                if user not in user_count:\n",
        "                    user_count[user] = 1\n",
        "                else:\n",
        "                    user_count[user] += 1\n",
        "\n",
        "                # Count the occurrences of each location\n",
        "                if loc not in loc_count:\n",
        "                    loc_count[loc] = 1\n",
        "                else:\n",
        "                    loc_count[loc] += 1\n",
        "\n",
        "    record_num = sum(user_count.values())\n",
        "    print(f'Finished, records: {record_num}, unique users: {len(user_count)}, unique locations: {len(loc_count)}')\n",
        "\n",
        "    # Filter and encode user and location\n",
        "    print('='*20, 'Filtering and encoding')\n",
        "\n",
        "    # Filter users based on the minimum number of records\n",
        "    for user in user_count:\n",
        "        if user_count[user] > params['user_record_min']:\n",
        "            user_id[user] = len(user_id)\n",
        "\n",
        "    # Filter locations based on the minimum number of records\n",
        "    for loc in loc_count:\n",
        "        if loc_count[loc] > params['loc_record_min']:\n",
        "            loc_id[loc] = len(loc_id)\n",
        "\n",
        "    filter_path = f'/content/dataset_TSMC2014_NYC_filtered.csv'\n",
        "    print(f'Filter path is {filter_path}')\n",
        "\n",
        "    # Write the filtered records to a new file\n",
        "    with open(filter_path, 'w', encoding='latin-1', newline='') as f_out:\n",
        "        fieldnames = [params['user_po'], params['loc_po']] + [col for col in reader.fieldnames if col not in [params['user_po'], params['loc_po']]]\n",
        "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        with open(filepath, 'r', encoding='latin-1') as f_in:\n",
        "            reader = csv.DictReader(f_in)\n",
        "            for record in reader:\n",
        "                user = record[params['user_po']]\n",
        "                loc = record[params['loc_po']]\n",
        "\n",
        "                # If the user and location are in the filtered list, encode them with unique IDs\n",
        "                if user in user_id and loc in loc_id:\n",
        "                    record[params['user_po']] = user_id[user]\n",
        "                    record[params['loc_po']] = loc_id[loc]\n",
        "                    writer.writerow(record)\n",
        "\n",
        "    record_num = sum(1 for _ in open(filter_path, 'r'))\n",
        "    print(f'Finished, records: {record_num}, unique users: {len(user_id)}, unique locations: {len(loc_id)}')\n",
        "\n",
        "    # Merge data\n",
        "    print('='*20, 'Merging')\n",
        "\n",
        "    merge_path = f'/content/dataset_TSMC2014_NYC_merged.csv'\n",
        "    print(f'Merge path is {merge_path}')\n",
        "\n",
        "    # Write the merged records to a new file\n",
        "    with open(merge_path, 'w', encoding='latin-1', newline='') as f_out:\n",
        "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        with open(filter_path, 'r', encoding='latin-1') as f_in:\n",
        "            reader = csv.DictReader(f_in)\n",
        "            pre_record = next(reader)\n",
        "            writer.writerow(pre_record)\n",
        "            for record in reader:\n",
        "\n",
        "                # Skip the record if it has the same user, location, and date as the previous record\n",
        "                if record[params['user_po']] == pre_record[params['user_po']] and \\\n",
        "                   record[params['loc_po']] == pre_record[params['loc_po']] and \\\n",
        "                   record[params['tim_po']].split('T')[0] == pre_record[params['tim_po']].split('T')[0]:\n",
        "                    continue\n",
        "\n",
        "                writer.writerow(record)\n",
        "                pre_record = record\n",
        "\n",
        "    record_num = sum(1 for _ in open(merge_path, 'r'))\n",
        "    print(f'Finished, records: {record_num}')\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "preprocessing()\n",
        "print('Time cost:', f'{time.time()-start_time:.0f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize dictionaries to store location and user information.\n",
        "Load the file and count the occurrences of each user and location.Filter users and locations based on the minimum number of records specified in the parameters.Write the filtered records to a new file, encoding the user and location with unique IDs.Write the merged records to a new file, skipping records that have the same user, location, and date as the previous record."
      ],
      "metadata": {
        "id": "9m0kYSj-gHnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "from math import pi\n",
        "from collections import Counter\n",
        "\n",
        "class DataGeneration(object):\n",
        "    def __init__(self, params):\n",
        "        self.__dict__.update(params.__dict__)\n",
        "\n",
        "        self.raw_data = {}     # raw user's trajectory. {uid: [[pid, tim], ...]}\n",
        "        self.poi_count = {}   # raw location counts. {pid: count}\n",
        "        self.data_filtered = {}\n",
        "        self.uid_list = []   # filtered user id\n",
        "        self.pid_dict = {}   # filtered location id map\n",
        "        self.train_data = {} # train data with history,   {'uid': {'sid': {'loc': [], 'tim': [], 'target': [] (, 'cat': [])}}}\n",
        "        self.train_id = {}   # train data session id list\n",
        "        self.test_data = {}\n",
        "        self.test_id = {}\n",
        "        self.tim_w = set()\n",
        "        self.tim_h = set()\n",
        "\n",
        "        self.raw_lat_lon = {} # count for latitude and longitude\n",
        "        self.new_lat_lon = {}\n",
        "        self.lat_lon_radians = {}\n",
        "\n",
        "\n",
        "        self.raw_cat_dict = {}  #  cid-cat\n",
        "        self.new_cat_dict = {}\n",
        "        self.pid_cat_dict = {}    # pid-cat dict\n",
        "\n",
        "    # 1. read trajectory data\n",
        "    def load_trajectory(self):\n",
        "        with open('/content/dataset_TSMC2014_NYC_merged.csv', 'r') as csv_file:\n",
        "            reader = csv.reader(csv_file)\n",
        "            for i, row in enumerate(reader):\n",
        "                userId, venueId, venueCategoryId, venueCategory, latitude, longitude, _, timezoneOffset = row\n",
        "                if self.cat_contained:\n",
        "                    # count uid records\n",
        "                    if userId not in self.raw_data:\n",
        "                        self.raw_data[userId] = [[venueId, timezoneOffset, venueCategory]]\n",
        "                    else:\n",
        "                        self.raw_data[userId].append([venueId, timezoneOffset, venueCategory])\n",
        "                    # count raw_venueCategoryId-cat\n",
        "                    if venueCategoryId not in self.raw_cat_dict:\n",
        "                        self.raw_cat_dict[venueCategoryId] = venueCategory\n",
        "                else:\n",
        "                    if userId not in self.raw_data:\n",
        "                        self.raw_data[userId] = [[venueId, timezoneOffset]]\n",
        "                    else:\n",
        "                        self.raw_data[userId].append([venueId, timezoneOffset])\n",
        "                if venueId not in self.poi_count:\n",
        "                    self.poi_count[venueId] = 1\n",
        "                else:\n",
        "                    self.poi_count[venueId] += 1\n",
        "\n",
        "                # count poi latitude and longitude\n",
        "                if venueId not in self.raw_lat_lon:\n",
        "                    self.raw_lat_lon[venueId] = [eval(latitude), eval(longitude)]\n",
        "\n",
        "    # 2. filter users and locations, and then split trajectory into sessions\n",
        "    def filter_and_divide_sessions(self):\n",
        "        POI_MIN_RECORD_FOR_USER = 1  # keep same setting with DeepMove and LSTPM\n",
        "\n",
        "        # filter user and location\n",
        "        uid_list = [x for x in self.raw_data if len(self.raw_data[x]) > self.user_record_min]      # uid list\n",
        "        pid_list = [x for x in self.poi_count if self.poi_count[x] > self.poi_record_min]  # pid list\n",
        "\n",
        "        # iterate each user\n",
        "        for uid in uid_list:\n",
        "            user_records = self.raw_data[uid]\n",
        "            user_records.sort(key=lambda x: x[1])  # sort records by timestamp\n",
        "\n",
        "            valid_session_flag = False   # session validation flag\n",
        "            valid_sessions = []   # valid sessions\n",
        "            sessions = []\n",
        "\n",
        "            for i in range(len(user_records) - 1):\n",
        "                # divide sessions by time\n",
        "                current_record = user_records[i]\n",
        "                next_record = user_records[i+1]\n",
        "                sessions.append(current_record)\n",
        "\n",
        "                # calculate time interval between two consecutive records\n",
        "                current_time = datetime.datetime.strptime(current_record[1], self.time_format)\n",
        "                next_time = datetime.datetime.strptime(next_record[1], self.time_format)\n",
        "                time_interval = (next_time - current_time).total_seconds() / 60  # convert to minutes\n",
        "\n",
        "                # split sessions when time interval exceeds session threshold\n",
        "                if time_interval > self.session_threshold:\n",
        "                    sessions.append(next_record)\n",
        "                    if len(sessions) >= self.session_minlen:\n",
        "                        valid_session_flag = True\n",
        "                        valid_sessions.extend(sessions)\n",
        "                    sessions = []\n",
        "\n",
        "            # check if the last session is valid\n",
        "            last_record = user_records[-1]\n",
        "            if len(sessions) == 0 or sessions[-1] != last_record:\n",
        "                sessions.append(last_record)\n",
        "                if len(sessions) >= self.session_minlen:\n",
        "                    valid_session_flag = True\n",
        "                    valid_sessions.extend(sessions)\n",
        "\n",
        "            if valid_session_flag:\n",
        "                self.data_filtered[uid] = valid_sessions\n",
        "\n",
        "        # generate filtered user list\n",
        "        self.uid_list = list(self.data_filtered.keys())\n",
        "\n",
        "        # generate location id map\n",
        "        for i, pid in enumerate(pid_list):\n",
        "            self.pid_dict[pid] = i + 1\n",
        "\n",
        "        # re-encode location indices in the filtered data\n",
        "        for uid, sessions in self.data_filtered.items():\n",
        "            filtered_sessions = []\n",
        "            for session in sessions:\n",
        "                pid = session[0]\n",
        "                if pid in self.pid_dict:\n",
        "                    session[0] = self.pid_dict[pid]\n",
        "                    filtered_sessions.append(session)\n",
        "            if filtered_sessions:\n",
        "                self.data_filtered[uid] = filtered_sessions\n",
        "\n",
        "        # generate pid-cat mapping dict\n",
        "        for pid, cat in self.raw_cat_dict.items():\n",
        "            self.pid_cat_dict[pid] = cat\n",
        "\n",
        "        # update poi_count based on filtered data\n",
        "        self.poi_count = Counter([record[0] for sessions in self.data_filtered.values() for record in sessions])\n",
        "\n",
        "    # 3. preprocess data and split into train/test sets\n",
        "    def preprocess_and_split(self):\n",
        "        # calculate latitude and longitude ranges\n",
        "        lat_list = []\n",
        "        lon_list = []\n",
        "\n",
        "        for lat, lon in self.raw_lat_lon.values():\n",
        "            if isinstance(lat, (float, int)) and isinstance(lon, (float, int)):\n",
        "                lat_list.append(lat)\n",
        "                lon_list.append(lon)\n",
        "\n",
        "        lat_min, lat_max = min(lat_list), max(lat_list)\n",
        "        lon_min, lon_max = min(lon_list), max(lon_list)\n",
        "\n",
        "\n",
        "        # calculate latitude and longitude ranges in radians\n",
        "        lat_min_rad = lat_min * (pi / 180)\n",
        "        lat_max_rad = lat_max * (pi / 180)\n",
        "        lon_min_rad = lon_min * (pi / 180)\n",
        "        lon_max_rad = lon_max * (pi / 180)\n",
        "\n",
        "        # preprocess latitude and longitude values\n",
        "        for pid, (lat, lon) in self.raw_lat_lon.items():\n",
        "            try:\n",
        "                lat_rad = float(lat) * (pi / 180)\n",
        "                lon_rad = float(lon) * (pi / 180)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            # normalize latitude and longitude to the range [0, 1]\n",
        "            lat_normalized = (lat_rad - lat_min_rad) / (lat_max_rad - lat_min_rad)\n",
        "            lon_normalized = (lon_rad - lon_min_rad) / (lon_max_rad - lon_min_rad)\n",
        "\n",
        "            self.new_lat_lon[pid] = [lat_normalized, lon_normalized]\n",
        "            self.lat_lon_radians[pid] = [lat_rad, lon_rad]\n",
        "\n",
        "        # split sessions into train and test sets\n",
        "        for uid, sessions in self.data_filtered.items():\n",
        "            num_sessions = len(sessions)\n",
        "            num_train = int(num_sessions * self.train_ratio)\n",
        "\n",
        "            train_sessions = sessions[:num_train]\n",
        "            test_sessions = sessions[num_train:]\n",
        "\n",
        "            if len(train_sessions) >= self.session_minlen:\n",
        "                self.train_data[uid] = self.generate_samples(train_sessions)\n",
        "                self.train_id[uid] = [session[0] for session in train_sessions]\n",
        "\n",
        "            if len(test_sessions) >= self.session_minlen:\n",
        "                self.test_data[uid] = self.generate_samples(test_sessions)\n",
        "                self.test_id[uid] = [session[0] for session in test_sessions]\n",
        "\n",
        "    def generate_samples(self, sessions):\n",
        "      samples = {'loc': [], 'tim': [], 'target': []}\n",
        "      session_id = sessions[0][0]\n",
        "      samples['loc'].append(session_id)\n",
        "      samples['tim'].append(0)\n",
        "      samples['target'].append(0)\n",
        "\n",
        "      for session in sessions:\n",
        "          loc, tim, category = session[0], session[1], session[2]  # Include the venue category\n",
        "          samples['loc'].append(category)  # Use the category instead of loc\n",
        "          samples['tim'].append(tim)\n",
        "          samples['target'].append(category)  # Use the category instead of loc\n",
        "\n",
        "      return samples\n",
        "\n",
        "\n",
        "    def save_preprocessed_data(self):\n",
        "      with open('/content/dataset_TSMC2014_NYC_preprocessed.csv', 'w', newline='') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerow(['uid', 'sid', 'loc', 'tim', 'target'])  # Write header\n",
        "\n",
        "          for uid, sessions in self.train_data.items():\n",
        "              for i in range(len(sessions['loc'])):\n",
        "                  if (\n",
        "                      i < len(self.train_id[uid])\n",
        "                      and i < len(sessions['loc'])\n",
        "                      and i < len(sessions['tim'])\n",
        "                      and i < len(sessions['target'])\n",
        "                  ):\n",
        "                      loc = sessions['loc'][i]\n",
        "                      target = sessions['target'][i]\n",
        "                      writer.writerow([uid, self.train_id[uid][i], loc, sessions['tim'][i], target])\n",
        "\n",
        "          for uid, sessions in self.test_data.items():\n",
        "              for i in range(len(sessions['loc'])):\n",
        "                  if (\n",
        "                      i < len(self.test_id[uid])\n",
        "                      and i < len(sessions['loc'])\n",
        "                      and i < len(sessions['tim'])\n",
        "                      and i < len(sessions['target'])\n",
        "                  ):\n",
        "                      loc = sessions['loc'][i]\n",
        "                      target = sessions['target'][i]\n",
        "                      writer.writerow([uid, self.test_id[uid][i], loc, sessions['tim'][i], target])\n",
        "\n",
        "\n",
        "def main():\n",
        "    params = argparse.Namespace(\n",
        "        data_name='NYC',\n",
        "        train_ratio=0.8,\n",
        "        session_threshold=30,\n",
        "        session_minlen=2,\n",
        "        user_record_min=1,\n",
        "        poi_record_min=1,\n",
        "        cat_contained=True,\n",
        "        time_format=\"%a %b %d %H:%M:%S +0000 %Y\"\n",
        "    )\n",
        "    # Create an instance of DataGeneration class with the params dictionary\n",
        "    data_generation = DataGeneration(params)\n",
        "    data_generation.load_trajectory()\n",
        "    data_generation.filter_and_divide_sessions()\n",
        "    data_generation.preprocess_and_split()\n",
        "    data_generation.save_preprocessed_data()\n",
        "\n",
        "    print(\"Sample Data:\")\n",
        "    print(\"User IDs:\", data_generation.uid_list[:5])\n",
        "    print(\"Location ID Map:\", data_generation.pid_dict)\n",
        "    print(\"Train Data:\", data_generation.train_data[data_generation.uid_list[0]])\n",
        "    print(\"Test Data:\", data_generation.test_data[data_generation.uid_list[0]])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    preprocessing()\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bLHVPSBE-NLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load Trajectory Data: The load_trajectory method reads the trajectory data from a CSV file (dataset_TSMC2014_NYC_merged.csv). It iterates over the records and extracts information such as user ID, venue ID, venue category ID, latitude, longitude, and timezone offset. The data is stored in various dictionaries to keep track of raw data, location counts, category mappings, etc.\n",
        "\n",
        "Step 2: Filter and Divide Sessions: The filter_and_divide_sessions method filters the users and locations based on certain criteria (minimum user records and minimum location records). It then divides the filtered user trajectories into sessions based on the session threshold (maximum time gap between consecutive records) and session minimum length. Valid sessions are stored in the data_filtered dictionary.\n",
        "\n",
        "Step 3: Preprocess and Split Data: The preprocess_and_split method preprocesses the data and splits it into train and test sets. It calculates latitude and longitude ranges, preprocesses latitude and longitude values by normalizing them, and stores the new latitude-longitude mappings. Then, it iterates over the filtered user trajectories and splits them into train and test sessions based on the train ratio provided in the params. For each user, samples are generated by converting location IDs to category IDs and creating target sequences. The train and test data are stored in the train_data and test_data dictionaries, respectively.\n",
        "\n",
        "Saving Preprocessed Data: The save_preprocessed_data method saves the preprocessed data into a CSV file (dataset_TSMC2014_NYC_preprocessed.csv). It writes the header and iterates over the train and test data, writing the user ID, session ID, location, timestamp, and target into the file."
      ],
      "metadata": {
        "id": "OwcbueXkk-JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/dataset_TSMC2014_NYC_preprocessed.csv')\n",
        "\n",
        "# Sort the dataset by user and timestamp\n",
        "dataset = dataset.sort_values(['uid', 'tim'])\n",
        "\n",
        "# Create 'Current Location' and 'Next Location' columns\n",
        "dataset['Current Location'] = dataset['loc']\n",
        "dataset['Next Location'] = dataset.groupby('uid')['loc'].shift(-1)\n",
        "\n",
        "# Drop the rows with missing next locations\n",
        "dataset = dataset.dropna(subset=['Next Location'])\n",
        "\n",
        "# Convert tim column to datetime with error handling\n",
        "dataset['tim'] = pd.to_datetime(dataset['tim'], errors='coerce')\n",
        "\n",
        "# Remove rows with invalid tims\n",
        "dataset = dataset.dropna(subset=['tim'])\n",
        "\n",
        "# Extract features from tim\n",
        "dataset['Hour'] = dataset['tim'].dt.hour\n",
        "dataset['Minute'] = dataset['tim'].dt.minute\n",
        "dataset['DayOfWeek'] = dataset['tim'].dt.dayofweek\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['Current Location'] = label_encoder.fit_transform(dataset['Current Location'])\n",
        "dataset['Next Location'] = label_encoder.transform(dataset['Next Location'])\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = dataset[['Current Location', 'Hour', 'Minute', 'DayOfWeek']]\n",
        "y = dataset['Next Location']\n",
        "\n",
        "# Split the encoded features and target into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using Min-Max scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(1, X_train.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "pU5SKyN-XyGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transform the numerical predictions to categorical labels\n",
        "predicted_labels = label_encoder.inverse_transform(predictions.flatten().astype(int))\n",
        "\n",
        "# Print the predicted labels (locations)\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-0O8IHkdoZv",
        "outputId": "8ed404a7-4878-4a12-de62-632f2aad1b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Miscellaneous Shop' 'Miscellaneous Shop' 'Hotel' ... 'Market'\n",
            " 'General Travel' 'German Restaurant']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current time information\n",
        "now = datetime.now()\n",
        "hour = now.hour\n",
        "minute = now.minute\n",
        "day_of_week = now.weekday()\n",
        "\n",
        "# Preprocess the input location\n",
        "input_location = 'Food & Drink Shop'\n",
        "preprocessed_location = label_encoder.transform([input_location])[0]\n",
        "\n",
        "# Create a sample input with relevant features\n",
        "sample_input = np.array([[preprocessed_location, hour, minute, day_of_week]])\n",
        "\n",
        "# Scale the features\n",
        "sample_input = scaler.transform(sample_input)\n",
        "\n",
        "# Reshape the sample input\n",
        "sample_input = np.reshape(sample_input, (sample_input.shape[0], 1, sample_input.shape[1]))\n",
        "\n",
        "# Use the trained model to predict the next location\n",
        "predicted_next_location = model.predict(sample_input)\n",
        "\n",
        "# Inverse transform the predicted numerical value to the original categorical label\n",
        "predicted_next_location_label = label_encoder.inverse_transform(predicted_next_location.flatten().astype(int))\n",
        "\n",
        "# Get the predicted probabilities for each location\n",
        "predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "# Get the indices of the top 5 highest probabilities excluding the predicted next location\n",
        "top_5_indices = np.argsort(-predicted_probabilities.flatten())[:6]  # Increase the range to 6 to exclude the current location\n",
        "\n",
        "# Filter out unseen labels and the predicted next location, and get the corresponding predicted next locations\n",
        "top_5_next_locations = []\n",
        "for index in top_5_indices:\n",
        "    if index < len(label_encoder.classes_) and index != predicted_next_location:\n",
        "        next_location = label_encoder.inverse_transform([index])[0]\n",
        "        top_5_next_locations.append(next_location)\n",
        "\n",
        "# Print the predicted next location and the next 5 most probable locations\n",
        "print('Predicted Next Location:', predicted_next_location_label)\n"
      ],
      "metadata": {
        "id": "dwDX65RjeFVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the preprocessed dataset from the CSV file.\n",
        "Sort the dataset by user and timestamp to ensure chronological order of the records.\n",
        "Create 'Current Location' and 'Next Location' columns based on the 'loc' column. These columns represent the current and subsequent locations for each user.\n",
        "Drop the rows with missing next locations (last location for each user).\n",
        "Convert the 'tim' column to datetime format with error handling.\n",
        "Remove rows with invalid timestamps.\n",
        "Extract additional features from the 'tim' column, such as hour, minute, and day of the week.\n",
        "Encode the categorical features ('Current Location' and 'Next Location') using LabelEncoder to convert them into numeric values.\n",
        "Split the dataset into features (X) and target (y).\n",
        "Split the encoded features and target into train and test sets using train_test_split.\n",
        "Scale the features using Min-Max scaler to normalize the values.\n",
        "Reshape the input data for the LSTM model by adding an additional dimension.\n",
        "Build the LSTM model using the Sequential model from Keras. The model consists of an LSTM layer with 64 units and a Dense layer with 1 unit.\n",
        "Compile the model with mean squared error (MSE) as the loss function and Adam optimizer.\n",
        "Train the model on the training data for 10 epochs with a batch size of 32.\n",
        "Evaluate the trained model on the test set and calculate the loss.\n",
        "Make predictions using the trained model on the test set."
      ],
      "metadata": {
        "id": "RZxHh7AHlT0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted next location\n",
        "predicted_next_location = model.predict(X_test)\n",
        "\n",
        "# Convert predicted labels to categorical values\n",
        "predicted_labels = label_encoder.inverse_transform(predicted_next_location.flatten().astype(int))\n",
        "\n",
        "# Convert actual labels to categorical values\n",
        "actual_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (predicted_labels == actual_labels).mean() * 100\n",
        "\n",
        "# Print accuracy\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5BbIyyPtx_X",
        "outputId": "37c57249-eaf0-4574-8da4-9cae269bc02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "872/872 [==============================] - 2s 3ms/step\n",
            "Accuracy: 0.8605234851201148\n"
          ]
        }
      ]
    }
  ]
}